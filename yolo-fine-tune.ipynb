{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc3bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = \"D:/Workforce_Attire/.cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c4d571-6dd4-4818-9625-145db50ccb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30d71e43-83f7-4c4c-ae70-3a84710cf6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/.pyenv/versions/3.11.8/lib/python3.11/site-packages/transformers/models/yolos/feature_extraction_yolos.py:28: FutureWarning: The class YolosFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use YolosImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = transformers.YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\n",
    "model = transformers.YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14f49f48-01b2-4a2b-8467-e1ca533f7179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YolosFeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_pad\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"format\": \"coco_detection\",\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"YolosFeatureExtractor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"longest_edge\": 1333,\n",
       "    \"shortest_edge\": 512\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb3be9ca-0e42-4078-9319-df71d9f046e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "es = datasets.load_dataset(\"adam-narozniak/clothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef12d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(ex):\n",
    "    return feature_extractor(ex[\"image\"], return_tensors=\"pt\", size={\"height\": 800, \"width\": 800})\n",
    "\n",
    "\n",
    "def map_to_labels(example):\n",
    "    objects = example[\"objects\"]\n",
    "    width = example[\"width\"]\n",
    "    height = example[\"height\"]\n",
    "    return {\n",
    "        \"label_ids\":\n",
    "            {\n",
    "                \"class_labels\": objects[\"category\"],\n",
    "                \"boxes\":  center_bbox_xy(rescale_bboxes_to_img(objects[\"bbox\"], width, height)), # dont know if center_bbox_xy is needed\n",
    "                #\"area\":  objects[\"area\"],\n",
    "                #\"iscrowd\": torch.Tensor([0]),\n",
    "                #\"orig_size\": torch.Tensor([width, height]).int(),\n",
    "                #\"size\": torch.Tensor([example['pixel_values'].shape[1:]])[0].int() dont know if needed\n",
    "            } \n",
    "   }\n",
    "    \n",
    "small_train_dataset = es[\"train\"].shuffle(seed=42).select(range(100)).map(extractor, batched=True, batch_size=5).with_format(type=\"pt\", columns=['pixel_values'], output_all_columns=True).map(map_to_labels)\n",
    "small_eval_dataset = es[\"train\"].shuffle(seed=24).select(range(10)).map(extractor, batched=True, batch_size=5).with_format(type=\"pt\", columns=['pixel_values'], output_all_columns=True).map(map_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aa2f3a1-972d-42b1-a302-2271ce9cee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(ex):\n",
    "    return feature_extractor(ex[\"image\"], return_tensors=\"pt\", size={\"height\": 800, \"width\": 800})\n",
    "\n",
    "\n",
    "def map_to_labels(example):\n",
    "    objects = example[\"objects\"]\n",
    "    return {\n",
    "        \"label_ids\":\n",
    "            {\n",
    "                \"class_labels\": objects[\"category\"],\n",
    "                \"boxes\":  objects[\"bbox\"]\n",
    "            } \n",
    "   }\n",
    "    \n",
    "small_train_dataset = es[\"train\"].shuffle(seed=42).select(range(1000)).map(extractor, batched=True, batch_size=5).map(map_to_labels).with_format(type=\"pt\", columns=['pixel_values'], output_all_columns=True)\n",
    "small_eval_dataset = es[\"train\"].shuffle(seed=24).select(range(100)).map(extractor, batched=True, batch_size=5).map(map_to_labels).with_format(type=\"pt\", columns=['pixel_values'], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "589e73b3-1e66-4e62-9d1a-60b935d29c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df058c88-7c8e-4830-a7a7-25e107806ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def YOLODataCollator(inputs):\n",
    "    return {\n",
    "        \"pixel_values\": torch.stack([el[\"pixel_values\"] for el in inputs]),\n",
    "        \"labels\": [\n",
    "            {                \n",
    "                \"class_labels\": torch.tensor(el[\"label_ids\"][\"class_labels\"]),\n",
    "                \"boxes\":  torch.tensor(el[\"label_ids\"][\"boxes\"]).float()\n",
    "            } \n",
    "            for el in inputs\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5ce207-2813-49f8-8b2f-b84a62ffee29",
   "metadata": {},
   "source": [
    "labels (List[Dict] of len (batch_size,), optional) â€” Labels for computing the bipartite matching loss. List of dicts, each dictionary containing at least the following 2 keys: 'class_labels' and 'boxes' (the class labels and bounding boxes of an image in the batch respectively). The class labels themselves should be a torch.LongTensor of len (number of bounding boxes in the image,) and the boxes a torch.FloatTensor of shape (number of bounding boxes in the image, 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d4009d8-95c8-4489-984c-92d7e1c0dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"fine-tune\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    label_names=[\"label_ids\"],\n",
    "    per_device_train_batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49b0fd1b-902c-4e12-a8d4-b86bed3f4e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    data_collator=YOLODataCollator,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8261dca-5314-4ee8-82ad-46f3378bdf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/.pyenv/versions/3.11.8/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 06:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12461.822000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12456.442000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>12453.371000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=12457.211666666666, metrics={'train_runtime': 388.6326, 'train_samples_per_second': 7.719, 'train_steps_per_second': 3.86, 'total_flos': 2.2425071616e+17, 'train_loss': 12457.211666666666, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2285046-a2c3-46b7-8bf9-e40830db132e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04292804-b663-4080-a850-e2ab67640651",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_eval_dataset_pre = es[\"train\"].shuffle(seed=24).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b8d1ca4-bda8-4722-8e83-93ed0319d4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373a38fb5ea84c50a09e466816bf9768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1080x1080>,\n",
       " 'objects': {'bbox_id': [4439],\n",
       "  'category': [8],\n",
       "  'bbox': [[11, 50, 1055, 1025]],\n",
       "  'area': [1017900],\n",
       "  'genre': [None]},\n",
       " 'width': 1080,\n",
       " 'height': 1080,\n",
       " 'labels': [{'boxes': [11, 50, 1055, 1025], 'class_labels': 8}]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = small_eval_dataset_pre.map(map_to_labels)[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2548f15-2040-454f-8327-d3cf4b6451a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[-0.0458,  0.1597,  0.0912,  ...,  1.0159,  1.0331,  1.0331],\n",
       "          [-0.2856, -0.1314,  0.0227,  ...,  1.0502,  1.0673,  1.0673],\n",
       "          [-0.3027, -0.2684, -0.3198,  ...,  1.1015,  1.1187,  1.0844],\n",
       "          ...,\n",
       "          [ 1.5125,  1.5125,  1.5125,  ...,  0.3309,  0.8447,  1.2385],\n",
       "          [ 1.5125,  1.5125,  1.5125,  ...,  0.2624,  0.3652,  1.0673],\n",
       "          [ 1.5125,  1.5125,  1.5125,  ...,  0.4166,  0.3994,  0.9988]],\n",
       " \n",
       "         [[-0.5826, -0.3725, -0.4426,  ...,  1.1506,  1.1856,  1.1856],\n",
       "          [-0.8277, -0.6702, -0.5126,  ...,  1.1856,  1.2206,  1.2206],\n",
       "          [-0.8452, -0.8102, -0.8452,  ...,  1.2381,  1.2731,  1.2381],\n",
       "          ...,\n",
       "          [ 1.6758,  1.6758,  1.6758,  ...,  0.2752,  0.8004,  1.2031],\n",
       "          [ 1.6758,  1.6758,  1.6758,  ...,  0.2052,  0.3102,  1.0280],\n",
       "          [ 1.6758,  1.6758,  1.6758,  ...,  0.3627,  0.3452,  0.9580]],\n",
       " \n",
       "         [[-0.5147, -0.3230, -0.4101,  ...,  1.3328,  1.4025,  1.4374],\n",
       "          [-0.7936, -0.6541, -0.4973,  ...,  1.3677,  1.4374,  1.4722],\n",
       "          [-0.8458, -0.8284, -0.8807,  ...,  1.4025,  1.4722,  1.4897],\n",
       "          ...,\n",
       "          [ 1.8905,  1.8905,  1.8905,  ...,  0.0431,  0.6531,  1.0888],\n",
       "          [ 1.8905,  1.8905,  1.8905,  ..., -0.0267,  0.1651,  0.9145],\n",
       "          [ 1.8905,  1.8905,  1.8905,  ...,  0.1302,  0.1999,  0.8448]]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9b399c3-c4d2-491d-87db-548285a6e9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1080x1080>,\n",
       " 'objects': {'bbox_id': [2, 3, 4],\n",
       "  'category': [13, 0, 4],\n",
       "  'bbox': [[587, 595, 674, 646], [591, 747, 663, 996], [504, 687, 796, 1099]],\n",
       "  'area': [4437, 17928, 120304],\n",
       "  'genre': ['woman', 'woman', 'woman']},\n",
       " 'width': 1080,\n",
       " 'height': 1080}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccd38d9e-1a53-411f-ba29-b3f888ae953c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          ...,\n",
       "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
       "\n",
       "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          ...,\n",
       "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
       "\n",
       "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          ...,\n",
       "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor(es[\"train\"][0][\"image\"], return_tensors=\"pt\", size={\"height\": 800, \"width\": 800})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742ea2f-0b1b-4c49-9424-f09829566afc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
