{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e5774e",
   "metadata": {},
   "source": [
    "AP, mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e049806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.2.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3754d35e4e564343962419016088bb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "full_data = datasets.load_from_disk(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c3602a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[ 0.5878,  0.6049,  0.6392,  ...,  0.5193,  0.3138,  0.1939],\n",
       "          [ 0.5536,  0.5878,  0.6392,  ...,  0.4679,  0.3481,  0.3309],\n",
       "          [ 0.6563,  0.6906,  0.7419,  ...,  0.4508,  0.3994,  0.3823],\n",
       "          ...,\n",
       "          [-1.4158, -1.1418, -1.0048,  ...,  0.8447,  0.8104,  0.7933],\n",
       "          [-0.9020, -0.9020, -1.1075,  ...,  0.7762,  0.7591,  0.7419],\n",
       "          [-0.8335, -0.9363, -1.0904,  ...,  0.5536,  0.4851,  0.4851]],\n",
       " \n",
       "         [[-0.6176, -0.6001, -0.5651,  ..., -0.8803, -1.0903, -1.2129],\n",
       "          [-0.6527, -0.6176, -0.5651,  ..., -0.8978, -1.0203, -1.0553],\n",
       "          [-0.5476, -0.5126, -0.4601,  ..., -0.8627, -0.9328, -0.9503],\n",
       "          ...,\n",
       "          [-1.3004, -1.0203, -0.8803,  ...,  1.7458,  1.7108,  1.6933],\n",
       "          [-0.7752, -0.7752, -0.9853,  ...,  1.6758,  1.6583,  1.6408],\n",
       "          [-0.7052, -0.8102, -0.9678,  ...,  1.4482,  1.3782,  1.3782]],\n",
       " \n",
       "         [[ 0.3742,  0.3916,  0.4265,  ...,  0.2173,  0.0082, -0.1138],\n",
       "          [ 0.3393,  0.3742,  0.4265,  ...,  0.1825,  0.0605,  0.0256],\n",
       "          [ 0.4439,  0.4788,  0.5311,  ...,  0.1476,  0.0779,  0.0779],\n",
       "          ...,\n",
       "          [-1.1770, -0.8981, -0.7587,  ...,  1.0191,  0.9494,  0.9145],\n",
       "          [-0.6541, -0.6541, -0.8633,  ...,  0.9494,  0.9145,  0.8797],\n",
       "          [-0.5844, -0.6890, -0.8458,  ...,  0.7228,  0.6356,  0.6182]]]),\n",
       " 'label_ids': {'boxes': tensor([[0.2037, 0.5270, 0.3778, 0.3593]]),\n",
       "  'class_labels': tensor([14])},\n",
       " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1080x1350>,\n",
       " 'objects': {'bbox_id': [295],\n",
       "  'category': [14],\n",
       "  'bbox': [[16, 469, 424, 954]],\n",
       "  'area': [197880],\n",
       "  'genre': ['man']},\n",
       " 'width': 1080,\n",
       " 'height': 1350}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d7080da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\lib\\site-packages\\transformers\\models\\yolos\\feature_extraction_yolos.py:38: FutureWarning: The class YolosFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use YolosImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "MODEL = transformers.YolosForObjectDetection.from_pretrained('checkpoint-44000', local_files_only=True) # from checkpoint\n",
    "PROCESSOR = transformers.YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\n",
    "\n",
    "\n",
    "def detect_boxes(image, threshold=0.9):\n",
    "    inputs = PROCESSOR(image, return_tensors=\"pt\", size={\"height\": 800, \"width\": 800})\n",
    "    outputs = MODEL(**inputs)\n",
    "\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = PROCESSOR.post_process_object_detection(outputs, threshold=threshold, target_sizes=target_sizes)[0]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "01c833c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 334/334 [16:46<00:00,  3.01s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATASET = full_data[\"test\"]\n",
    "\n",
    "IMAGE_PATH='metrics_data/images' # path to which we save images from the dataset\n",
    "ANNOTATIONS_PATH='metrics_data/annots' # path to which we save ground truths from the dataset\n",
    "DETECTION_PATH='metrics_data/dcs' # path to which we save predictions \n",
    "\n",
    "for path in [IMAGE_PATH, ANNOTATIONS_PATH, DETECTION_PATH]:\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "for i, data in enumerate(tqdm(DATASET)):\n",
    "    image = data['image']\n",
    "    image.save(os.path.join(IMAGE_PATH, f\"{i}.jpg\")) # saving image\n",
    "    results = detect_boxes(image) # model inference\n",
    "    annotations = data['label_ids'] \n",
    "    with open(os.path.join(ANNOTATIONS_PATH, f\"{i}.txt\"), \"w\") as f:\n",
    "        for box, label in zip(annotations['boxes'], annotations['class_labels']): # saving ground truths\n",
    "            f.write(f\"{label} {box[0]} {box[1]} {box[2]} {box[3]}\\n\")\n",
    "    \n",
    "    idxes = np.argsort(results['scores'].detach().numpy())[::-1] # indexes for sorting by prediction score\n",
    "    \n",
    "    with open(os.path.join(DETECTION_PATH, f\"{i}.txt\"), \"w\") as f:\n",
    "        for score, label, box in zip(results[\"scores\"].detach().numpy()[idxes],\\\n",
    "                                     results[\"labels\"].detach().numpy()[idxes],\\ \n",
    "                                     results[\"boxes\"].detach().numpy()[idxes]):\n",
    "            f.write(f\"{label} {score} {int(box[0])} {int(box[1])} {int(box[2])} {int(box[3])}\\n\") # saving predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1055c6c",
   "metadata": {},
   "source": [
    "Software used for computing metrics: [Object detection metrics](https://github.com/rafaelpadilla/review_object_detection_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
